---
title: ã€èˆ¹è¯´ã€‘æ·±åº¦å­¦ä¹ å…ˆå¯¼è¯¾ï¼ˆåŸã€Šä»0åˆ°1å¿«é€Ÿå…¥é—¨AIå¤§æ¨¡å‹ã€‹éƒ¨åˆ†è¯¾ç¨‹èµ„æºï¼‰
description: 
published: true
date: 2025-03-20T02:55:43.645Z
tags: 
editor: markdown
dateCreated: 2024-11-13T17:16:29.511Z
---

> :one: æœ¬è¯¾ç¨‹å±äºèˆ¹è¯´ç³»åˆ—è¯¾ç¨‹ä¹‹ä¸€ï¼Œ[ğŸ’°å•è¯¾è´­ä¹°ç‚¹è¿™é‡Œ](https://www.bilibili.com/cheese/play/ss3380){target="_blank"}
> :two: ä½ ä¹Ÿå¯ä»¥é€‰æ‹©è´­ä¹°ã€èˆ¹è¯´ç³»åˆ—è¯¾ç¨‹-å¹´åº¦ä¼šå‘˜ã€äº§å“ã€èˆ¹ç¥¨ã€ï¼Œç•…äº«æœŸå†…æ— é™åˆ¶å­¦ä¹ å·²ä¸Šçº¿çš„æ‰€æœ‰èˆ¹è¯´ç³»åˆ—è¯¾ç¨‹ï¼š
[ğŸ’° èˆ¹ç¥¨1å¹´æœŸè´­ä¹°å…¥å£](https://b23.tv/uCOhTk2){target="_blank"}
[ğŸ’° èˆ¹ç¥¨4å¹´æœŸè´­ä¹°å…¥å£](https://b23.tv/vU6TsQU){target="_blank"}
ä¼˜æƒ åˆ¸å¯ä»¥æ‰¾Biliå§ğŸ‘©â€ğŸ’»é¢†å–ã€‚å§çš„äºŒç»´ç ï¼š
![æ–°biliå§.png](/images/æ–°biliå§.png =100x)
{.is-success}


# ç¨‹åºæºç 
 
[ç¯å¢ƒä¾èµ–venv_arm.txt](ai_mllm_aigc/resource/venv_arm.txt)

1.4.py
```python
import numpy as np


# ================= åˆ›å»º =================
# ä»åˆ—è¡¨åˆ›å»º
arr_from_list = np.array([1, 2, 3, 4])
print("ä»åˆ—è¡¨åˆ›å»º:\n", arr_from_list)

# åˆ›å»ºå…¨é›¶æ•°ç»„
zeros_array = np.zeros((2, 3))
print("å…¨é›¶æ•°ç»„:\n", zeros_array)

# åˆ›å»ºå…¨ä¸€æ•°ç»„
ones_array = np.ones((3, 2))
print("å…¨ä¸€æ•°ç»„:\n", ones_array)

# åˆ›å»ºéšæœºæ•°ç»„
random_array = np.random.rand(2, 2)
print("éšæœºæ•°ç»„:\n", random_array)


# ================= åŸºæœ¬å±æ€§ =================
arr = np.array([[1, 2, 3], [4, 5, 6]])

print("æ•°ç»„å½¢çŠ¶:", arr.shape)
print("æ•°æ®ç±»å‹:", arr.dtype)
print("æ•°ç»„å¤§å°:", arr.size)
print("æ•°ç»„ç»´åº¦:", arr.ndim)


# ================= ç´¢å¼• =================
arr = np.array([10, 20, 30, 40, 50])

# è·å–å•ä¸ªå…ƒç´ 
print("è·å–ç¬¬äºŒä¸ªå…ƒç´ :", arr[1])

# è·å–å­æ•°ç»„
print("è·å–ç¬¬2åˆ°ç¬¬4ä¸ªå…ƒç´ :", arr[1:4])

# å¤šç»´ç´¢å¼•
arr_2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
print("è·å–ç¬¬äºŒè¡Œç¬¬ä¸‰åˆ—çš„å…ƒç´ :", arr_2d[1, 2])
print("è·å–ç¬¬ä¸€åˆ—çš„æ‰€æœ‰å…ƒç´ :", arr_2d[:, 0])


# ================= åŸºæœ¬è¿ç®— =================
arr1 = np.array([1, 2, 3])
arr2 = np.array([4, 5, 6])

# åŠ æ³•è¿ç®—
print("åŠ æ³•:", arr1 + arr2)  # è¾“å‡º [5 7 9]

# ä¹˜æ³•è¿ç®—
print("ä¹˜æ³•:", arr1 * arr2)  # è¾“å‡º [4 10 18]

# æ ‡é‡è¿ç®—
print("ä¹˜ä»¥æ ‡é‡:", arr1 * 2)  # è¾“å‡º [2 4 6]

# å¹¿æ’­æœºåˆ¶
arr3 = np.array([[1, 2, 3], [4, 5, 6]])
print("å¹¿æ’­åŠ æ³•:\n", arr3 + np.array([1, 2, 3]))  # æ¯è¡Œåˆ†åˆ«åŠ ä¸Š [1, 2, 3]


# ================= æ”¹å˜å½¢çŠ¶ =================
arr = np.array([[1, 2, 3], [4, 5, 6]])

# é‡å¡‘æ•°ç»„
reshaped_arr = arr.reshape(3, 2)
print("é‡å¡‘åçš„æ•°ç»„:\n", reshaped_arr)

print("è½¬ä¸ºåˆ—å‘é‡:\n", arr.reshape(-1, 1))

# å±•å¹³æ•°ç»„
flattened_arr = arr.flatten()
print("å±•å¹³åçš„æ•°ç»„:", flattened_arr)


# ================= ç»Ÿè®¡ =================
arr = np.array([[1, 2, 3], [4, 5, 6]])

print("æ€»å’Œ:", np.sum(arr))
print("æŒ‰åˆ—æ±‚å’Œ:", np.sum(arr, axis=0))
print("å‡å€¼:", np.mean(arr))
print("æ ‡å‡†å·®:", np.std(arr))
print("æœ€å¤§å€¼:", np.max(arr))
print("æœ€å°å€¼:", np.min(arr))


# ================= ä¸ list å¯¹æ¯” =================
import time

# ä½¿ç”¨NumPyè®¡ç®—
arr_np = np.arange(int(1e8))
start = time.time()
arr_np_sum = np.sum(arr_np)
print("NumPyæ±‚å’Œæ—¶é—´:", time.time() - start)

# ä½¿ç”¨Pythonåˆ—è¡¨è®¡ç®—
arr_list = list(range(int(1e8)))
start = time.time()
arr_list_sum = sum(arr_list)
print("Pythonåˆ—è¡¨æ±‚å’Œæ—¶é—´:", time.time() - start)

```
 1.5.py
 ```
 import torch
import numpy as np


# ================ åˆ›å»º ================
# æ‰‹åŠ¨åˆ›å»º
tensor_manual = torch.tensor([[1, 2], [3, 4]])
print("æ‰‹åŠ¨åˆ›å»ºtensor:\n", tensor_manual)

# å…¨é›¶ã€å…¨ä¸€å’Œéšæœºtensor
tensor_zeros = torch.zeros((2, 3))
tensor_ones = torch.ones((2, 3))
tensor_rand = torch.rand((2, 3))
print("å…¨é›¶tensor:\n", tensor_zeros)
print("å…¨ä¸€tensor:\n", tensor_ones)
print("éšæœºtensor:\n", tensor_rand)


# ================ åŸºç¡€å±æ€§ ================
tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32)

print("å½¢çŠ¶:", tensor.shape)
print("æ•°æ®ç±»å‹:", tensor.dtype)
print("è®¾å¤‡:", tensor.device)


# ================ è¿ç®— ================
# åŸºæœ¬åŠ æ³•è¿ç®—
tensor1 = torch.tensor([1.0, 2.0, 3.0])
tensor2 = torch.tensor([4.0, 5.0, 6.0])
print("åŠ æ³•:", tensor1 + tensor2)

# å¹¿æ’­æœºåˆ¶
tensor3 = torch.tensor([[1, 2], [3, 4]])
tensor_broadcast = tensor3 + torch.tensor([1, 2])
print("å¹¿æ’­åŠ æ³•:\n", tensor_broadcast)

# å°†tensorç§»åŠ¨åˆ°GPU
if torch.cuda.is_available():
    tensor_gpu = tensor1.to("cuda")
    print("åœ¨GPUä¸Šçš„tensor:", tensor_gpu)


# ================ æ”¹å˜å½¢çŠ¶ ================
tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])

# é‡å¡‘
reshaped_tensor = tensor.reshape(3, 2)
print("é‡å¡‘åçš„tensor:\n", reshaped_tensor)

# è½¬ç½®
transposed_tensor = tensor.transpose(0, 1)
print("è½¬ç½®åçš„tensor:\n", transposed_tensor)


# ================ è‡ªåŠ¨å¾®åˆ† ================
# åˆ›å»ºéœ€è¦æ¢¯åº¦çš„tensor
x = torch.tensor([2.0, 3.0], requires_grad=True)
y = x * x  # æ“ä½œ
y_sum = y.sum()  # æ±‚å’Œ
y_sum.backward()  # è®¡ç®—æ¢¯åº¦ï¼Œåªèƒ½å¯¹æ ‡é‡è°ƒç”¨backward()
print("xçš„æ¢¯åº¦:", x.grad)  # è¾“å‡º tensor([4., 6.])


# ================ ä¸ ndarray çš„è½¬æ¢ ================
# ä»ndarrayè½¬æ¢ä¸ºtensor
numpy_array = np.array([1, 2, 3])
tensor_from_numpy = torch.from_numpy(numpy_array)
print("ä»ndarrayè½¬æ¢ä¸ºtensor:", tensor_from_numpy)

# ä»tensorè½¬æ¢ä¸ºndarray
tensor = torch.tensor([4.0, 5.0, 6.0])
numpy_from_tensor = tensor.numpy()
print("ä»tensorè½¬æ¢ä¸ºndarray:", numpy_from_tensor)

 ```
 1.6.py
 ```
 import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt


# ===================== 1. æ„å»ºäººå·¥æ•°æ®é›† =====================
# å‡è®¾çœŸå®çš„å…³ç³»æ˜¯ y = 3x + 2ï¼Œå¹¶åŠ å…¥ä¸€äº›éšæœºå™ªå£°

# ç”Ÿæˆä»0åˆ°10çš„100ä¸ªç‚¹
X = torch.linspace(0, 10, 100)
# X.shape = (100)

X = X.unsqueeze(1)
# X.shape = (100, 1)

# ç”Ÿæˆ yï¼ŒåŠ å…¥éšæœºå™ªå£°
y = 3 * X + 2 + torch.randn(100, 1)
# y.shape = (100, 1)

# ç”»å‡ºç¤ºä¾‹æ•°æ®
plt.scatter(X.numpy(), y.numpy())
plt.show()


# ===================== 2. å®šä¹‰çº¿æ€§å›å½’æ¨¡å‹ =====================
class LinearRegressionModel(nn.Module):
    def __init__(self):
        super(LinearRegressionModel, self).__init__()
        
        # y = wx + bï¼Œè¾“å‡ºè¾“å‡ºéƒ½æ˜¯ 1 ç»´
        self.linear = nn.Linear(1, 1)

    def forward(self, x):
        return self.linear(x)

# å®ä¾‹åŒ–æ¨¡å‹
model = LinearRegressionModel()


# ===================== 3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ =====================
# å‡æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°
criterion = nn.MSELoss()

# éšæœºæ¢¯åº¦ä¸‹é™ (SGD) ä½œä¸ºä¼˜åŒ–å™¨
optimizer = optim.SGD(model.parameters(), lr=0.01)  # å­¦ä¹ ç‡ 0.01


# ===================== 4. è®­ç»ƒæ¨¡å‹ =====================
epochs = 1000  # è®­ç»ƒå›åˆæ•°
for epoch in range(epochs):
    # ä½¿ç”¨æ¨¡å‹é¢„æµ‹è¾“å‡º
    predictions = model(X)
    
    # è®¡ç®—æŸå¤±
    loss = criterion(predictions, y)
    
    # æ¢¯åº¦æ¸…é›¶ï¼Œé˜²æ­¢ç´¯ç§¯
    optimizer.zero_grad()
    
    # è®¡ç®—æ¢¯åº¦
    loss.backward()
    
    # æ›´æ–°å‚æ•°
    optimizer.step()

    # æ‰“å°è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å€¼
    if (epoch + 1) % 20 == 0:
        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')


# ===================== 5. å¯è§†åŒ–ç»“æœ =====================
# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹
predicted = model(X)

# åŸå§‹æ•°æ®
plt.scatter(X.numpy(), y.numpy(), label='Original Data')

# é¢„æµ‹æ•°æ®
plt.plot(X.numpy(), predicted.detach().numpy(), color='red', label='Fitted Line')

plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()

 ```
 1.9.py
 ```
 import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import torch.nn.functional as F


# ===================== 1. æ„å»ºäººå·¥æ•°æ®é›† =====================
# åˆ›å»ºä¸¤ä¸ªç±»åˆ«çš„æ•°æ®é›†ï¼Œä½¿ç”¨æ­£æ€åˆ†å¸ƒç”Ÿæˆä¸¤ç»„æ•°æ®

# ç±»åˆ« 0 çš„æ•°æ®ï¼Œå‡å€¼ä¸º (2, 2)
x0 = torch.randn(50, 2) + torch.tensor([2, 2])
# x0.shape = (50, 2)

# ç±»åˆ« 1 çš„æ•°æ®ï¼Œå‡å€¼ä¸º (7, 7)
x1 = torch.randn(50, 2) + torch.tensor([7, 7])

# æ ‡ç­¾ä¸º 0ã€1
y0 = torch.zeros(50, dtype=torch.long)
y1 = torch.ones(50, dtype=torch.long)  
# y0.shape = (50,)


# åˆå¹¶æ•°æ®
X = torch.cat([x0, x1], dim=0)
# X.shape = (100, 2)

y = torch.cat([y0, y1], dim=0)
# y.shape = (100,)

# å¯è§†åŒ–æ•°æ®
plt.scatter(X[:, 0].numpy(), X[:, 1].numpy(), c=y.numpy())
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Training Data')
plt.show()


# ===================== 2. å®šä¹‰Softmaxåˆ†ç±»æ¨¡å‹ =====================
class SoftmaxClassifier(nn.Module):
    def __init__(self):
        super(SoftmaxClassifier, self).__init__()
        
        # ä¸¤ä¸ªç‰¹å¾è¾“å…¥ï¼Œä¸¤ä¸ªç±»åˆ«è¾“å‡º
        self.linear = nn.Linear(2, 2)  # è¾“å…¥2ç»´ï¼Œè¾“å‡º2ç»´

    def forward(self, x):
        return self.linear(x)

# å®ä¾‹åŒ–æ¨¡å‹
model = SoftmaxClassifier()


# ===================== 3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ =====================
# äº¤å‰ç†µæŸå¤±ï¼Œå†…éƒ¨åŒ…å«äº† softmax
criterion = nn.CrossEntropyLoss()

# éšæœºæ¢¯åº¦ä¸‹é™ (SGD) ä½œä¸ºä¼˜åŒ–å™¨
optimizer = optim.SGD(model.parameters(), lr=0.01)  # å­¦ä¹ ç‡ 0.01


# ===================== 4. è®­ç»ƒæ¨¡å‹ =====================
epochs = 1000  # è®­ç»ƒå›åˆæ•°
for epoch in range(epochs):
    # ä½¿ç”¨æ¨¡å‹é¢„æµ‹è¾“å‡º
    predictions = model(X)
    # predictions.shape = (100, 2)
    
    # è®¡ç®—æŸå¤±
    loss = criterion(predictions, y)
    
    # æ¢¯åº¦æ¸…é›¶ï¼Œé˜²æ­¢ç´¯ç§¯
    optimizer.zero_grad()
    
    # è®¡ç®—æ¢¯åº¦
    loss.backward()
    
    # æ›´æ–°å‚æ•°
    optimizer.step()

    # æ‰“å°è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å€¼
    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')


# ===================== 5. å¯è§†åŒ–ç»“æœ =====================
# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨åŒä¸€æ•°æ®ä¸Šè¿›è¡Œé¢„æµ‹
with torch.no_grad():
    predicted_labels = torch.argmax(model(X), dim=1)  # å–å‡ºæ¯ä¸ªæ ·æœ¬çš„æœ€é«˜åˆ†å¯¹åº”çš„ç±»åˆ«
    # predicted_labels.shape = (100,)

# å¯è§†åŒ–çœŸå€¼ä¸é¢„æµ‹ç»“æœå¯¹æ¯”
plt.scatter(X[:, 0].numpy(), X[:, 1].numpy(), c=y.numpy(), marker='o', label='True Labels')
plt.scatter(X[:, 0].numpy(), X[:, 1].numpy(), c=predicted_labels.numpy(), marker='x', label='Predicted Labels')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('True Labels vs Predicted Labels')
plt.legend(['True Labels (circles)', 'Predicted Labels (crosses)'])
plt.show()

 ```
 2.5.py
 ```
 import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt


# ===================== 1. æ„å»ºäººå·¥æ•°æ®é›† =====================
# å‡è®¾çœŸå®çš„å…³ç³»æ˜¯ y = sin(2x) + log(abs(x) + 1) - 0.05x^3ï¼Œå¹¶åŠ å…¥ä¸€äº›éšæœºå™ªå£°

# ç”Ÿæˆä»-4åˆ°4çš„500ä¸ªç‚¹
X = torch.linspace(-4, 4, 500)
# X.shape = (500)

X = X.unsqueeze(1)
# X.shape = (500, 1)

# ç”Ÿæˆ yï¼ŒåŠ å…¥éšæœºå™ªå£°
y = torch.sin(2 * X) + torch.log(torch.abs(X) + 1) - 0.05 * X**3 + torch.randn(500, 1) * 0.2
# y.shape = (500, 1)

# ç”»å‡ºç¤ºä¾‹æ•°æ®
plt.scatter(X.numpy(), y.numpy(), s=10)
plt.title("Complex Nonlinear Dataset")
plt.show()


# ===================== 2. å®šä¹‰ MLP æ¨¡å‹ =====================
class MLPModel(nn.Module):
    def __init__(self):
        super(MLPModel, self).__init__()
        
        # å¤šå±‚æ„ŸçŸ¥æœºï¼ŒåŒ…å«ä¸¤å±‚éšè—å±‚
        self.mlp = nn.Sequential(
            nn.Linear(1, 64),  # è¾“å…¥ç»´åº¦ä¸º1ï¼Œéšè—å±‚ç»´åº¦ä¸º64
            nn.ReLU(),         # æ¿€æ´»å‡½æ•° ReLU
            nn.Linear(64, 64), # éšè—å±‚ç»´åº¦ä¸º64
            nn.ReLU(),         # æ¿€æ´»å‡½æ•° ReLU
            nn.Linear(64, 1),  # è¾“å‡ºç»´åº¦ä¸º1
        )

    def forward(self, x):
        return self.mlp(x)

# å®ä¾‹åŒ–æ¨¡å‹
model = MLPModel()


# ===================== 3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ =====================
# å‡æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°
criterion = nn.MSELoss()

# Adam ä½œä¸ºä¼˜åŒ–å™¨
optimizer = optim.Adam(model.parameters(), lr=0.01)  # å­¦ä¹ ç‡ 0.01


# ===================== 4. è®­ç»ƒæ¨¡å‹ =====================
epochs = 1000  # è®­ç»ƒå›åˆæ•°
for epoch in range(epochs):
    # ä½¿ç”¨æ¨¡å‹é¢„æµ‹è¾“å‡º
    predictions = model(X)
    
    # è®¡ç®—æŸå¤±
    loss = criterion(predictions, y)
    
    # æ¢¯åº¦æ¸…é›¶ï¼Œé˜²æ­¢ç´¯ç§¯
    optimizer.zero_grad()
    
    # è®¡ç®—æ¢¯åº¦
    loss.backward()
    
    # æ›´æ–°å‚æ•°
    optimizer.step()

    # æ‰“å°è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å€¼
    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')


# ===================== 5. å¯è§†åŒ–ç»“æœ =====================
# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹
predicted = model(X)

# åŸå§‹æ•°æ®
plt.scatter(X.numpy(), y.numpy(), label='Original Data', s=10)

# é¢„æµ‹æ•°æ®
plt.plot(X.numpy(), predicted.detach().numpy(), color='red', label='Fitted Curve')

plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.title("Fitted Curve by MLP")
plt.show()

 ```
 2.6.py
 ```
 import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt


# ===================== 1. æ„å»ºäººå·¥æ•°æ®é›† =====================
# è‡ªå®šä¹‰æ•°æ®é›†ç±»
class NonlinearDataset(Dataset):
    def __init__(self, num_samples=5000):
        super(NonlinearDataset, self).__init__()
        
        # ç”Ÿæˆæ•°æ®
        self.X = torch.linspace(-4, 4, num_samples).unsqueeze(1)  # (num_samples, 1)
        self.y = (
            torch.sin(2 * self.X)
            + torch.log(torch.abs(self.X) + 1)
            - 0.05 * self.X**3
            + torch.randn(num_samples, 1) * 0.2
        )  # (num_samples, 1)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# å®ä¾‹åŒ–æ•°æ®é›†
dataset = NonlinearDataset(num_samples=5000000)

# åˆ›å»º DataLoaderï¼ŒæŒ‡å®šæ‰¹é‡å¤§å°å’Œæ˜¯å¦æ‰“ä¹±æ•°æ®
batch_size = 1000000
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)


# ===================== 2. å®šä¹‰ MLP æ¨¡å‹ =====================
class MLPModel(nn.Module):
    def __init__(self):
        super(MLPModel, self).__init__()
        
        # å¤šå±‚æ„ŸçŸ¥æœºï¼ŒåŒ…å«ä¸¤å±‚éšè—å±‚
        self.mlp = nn.Sequential(
            nn.Linear(1, 64),  # è¾“å…¥ç»´åº¦ä¸º1ï¼Œéšè—å±‚ç»´åº¦ä¸º64
            nn.ReLU(),         # æ¿€æ´»å‡½æ•° ReLU
            nn.Linear(64, 64), # éšè—å±‚ç»´åº¦ä¸º64
            nn.ReLU(),         # æ¿€æ´»å‡½æ•° ReLU
            nn.Linear(64, 1),  # è¾“å‡ºç»´åº¦ä¸º1
        )

    def forward(self, x):
        return self.mlp(x)

# å®ä¾‹åŒ–æ¨¡å‹
model = MLPModel()


# ===================== 3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ =====================
# å‡æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°
criterion = nn.MSELoss()

# Adam ä½œä¸ºä¼˜åŒ–å™¨
optimizer = optim.Adam(model.parameters(), lr=0.01)  # å­¦ä¹ ç‡ 0.01


# ===================== 4. è®­ç»ƒæ¨¡å‹ =====================
epochs = 20  # è®­ç»ƒå›åˆæ•°
for epoch in range(epochs):
    for X_batch, y_batch in dataloader:
        # ä½¿ç”¨æ¨¡å‹é¢„æµ‹è¾“å‡º
        predictions = model(X_batch)
        
        # è®¡ç®—æŸå¤±
        loss = criterion(predictions, y_batch)
        
        # æ¢¯åº¦æ¸…é›¶ï¼Œé˜²æ­¢ç´¯ç§¯
        optimizer.zero_grad()
        
        # è®¡ç®—æ¢¯åº¦
        loss.backward()
        
        # æ›´æ–°å‚æ•°
        optimizer.step()

    # æ‰“å°è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å€¼
    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')


# ===================== 5. å¯è§†åŒ–ç»“æœ =====================
# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹
with torch.no_grad():
    predictions = model(dataset.X)

# åŸå§‹æ•°æ®
plt.scatter(dataset.X.numpy(), dataset.y.numpy(), label='Original Data', s=10)

# é¢„æµ‹æ•°æ®
plt.plot(dataset.X.numpy(), predictions.numpy(), color='red', label='Fitted Curve')

plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.title("Fitted Curve by MLP with Batches")
plt.show()

 ```
 2.11
 [ä¸‹è½½å‹ç¼©åŒ…2.11.zip](/courses_resource/ai_mllm_aigc/resource/src/2.11.zip)
 3.1.py
 ```
 #  PIL æ˜¯ Python Imaging Library çš„ç¼©å†™ï¼Œæ˜¯ Python å®˜æ–¹çš„å›¾åƒå¤„ç†åº“
from PIL import Image
import numpy as np
import torch

# 1. è¯»å–å›¾ç‰‡ï¼Œå°†æ­¤è·¯å¾„æ›¿æ¢ä¸ºä½ çš„å›¾ç‰‡è·¯å¾„
image = Image.open('./data/img.png')

# 2. å°†å›¾ç‰‡è½¬æ¢ä¸ºnumpyçš„ndarrayæ ¼å¼
image_array = np.array(image)
print("Numpy ndarrayæ ¼å¼çš„å›¾ç‰‡æ•°æ®ï¼š")
print("æ•°æ®ç±»å‹ï¼š", image_array.dtype)
print("å½¢çŠ¶ï¼š", image_array.shape)

# 3. å°†numpyçš„ndarrayè½¬æ¢ä¸ºPyTorchçš„tensoræ ¼å¼
image_tensor = torch.from_numpy(image_array)
print("PyTorch tensoræ ¼å¼çš„å›¾ç‰‡æ•°æ®ï¼š")
print("æ•°æ®ç±»å‹ï¼š", image_tensor.dtype)
print("å½¢çŠ¶ï¼š", image_tensor.shape)

 ```
 3.2.py
 ```
 import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader


class SimpleMLP(nn.Module):
    def __init__(self):
        super(SimpleMLP, self).__init__()

        # ç¬¬ä¸€å±‚å…¨è¿æ¥å±‚ï¼Œè¾“å…¥784ï¼ˆ28x28å›¾åƒï¼‰ï¼Œè¾“å‡º256
        self.fc1 = nn.Linear(784, 256)

        # ç¬¬äºŒå±‚å…¨è¿æ¥å±‚ï¼Œè¾“å…¥256ï¼Œè¾“å‡º128
        self.fc2 = nn.Linear(256, 128)

        # ç¬¬ä¸‰å±‚å…¨è¿æ¥å±‚ï¼Œè¾“å…¥128ï¼Œè¾“å‡º10ï¼ˆç±»åˆ«æ•°ï¼‰
        self.fc3 = nn.Linear(128, 10)

    def forward(self, x):
        # x.shape = (b, 1, 28, 28)
        
        # å°†å›¾åƒå±•å¹³ä¸ºä¸€ç»´å‘é‡
        x = torch.flatten(x, 1)
        # x.shape = (b, 784)

        # ç¬¬ä¸€å±‚å…¨è¿æ¥åä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°
        x = F.relu(self.fc1(x))

        # ç¬¬äºŒå±‚å…¨è¿æ¥åä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°
        x = F.relu(self.fc2(x))

        # ç¬¬ä¸‰å±‚å…¨è¿æ¥è¾“å‡º
        x = self.fc3(x)
        # x.shape = (b, 10)

        # åœ¨ä½¿ç”¨nn.CrossEntropyLossæ—¶ï¼Œä¸éœ€è¦åœ¨è¿™é‡Œåº”ç”¨Softmax
        return x

def main():
    # åŠ è½½ MNIST è®­ç»ƒé›†
    # å‚æ•°ï¼šæ•°æ®é›†çš„æœ¬åœ°è·¯å¾„ã€ä½¿ç”¨è®­ç»ƒé›†è¿˜æ˜¯æµ‹è¯•é›†ã€æ˜¯å¦è‡ªåŠ¨ä¸‹è½½æ•°æ®é›†ã€æ•°æ®é¢„å¤„ç†æµç¨‹
    train_dataset = datasets.MNIST(
        root='./data',
        train=True, 
        download=True, 
        transform=transforms.ToTensor()
    )
    
    # ä»æ•°æ®é›†åˆ›å»ºDataLoader
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

    model = SimpleMLP()
    
    # åˆ†ç±»é—®é¢˜ï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°
    loss_func = nn.CrossEntropyLoss()

    optimizer = optim.SGD(model.parameters(), lr=0.01)

    # è®¾ç½®ç½‘ç»œä¸ºè®­ç»ƒæ¨¡å¼
    model.train()

    for epoch in range(5):  # æ€»å…±è®­ç»ƒ5è½®
        # æœ¬è½®è®­ç»ƒçš„å¹³å‡loss
        train_loss = 0

        # enumerateç”¨æ³•ï¼šhttps://www.runoob.com/python/python-func-enumerate.html
        for batch_idx, (data, target) in enumerate(train_loader):
            # data.shape = (b, 1, 28, 28)
            # target.shape = (b, 10)

            # å°†ç½‘ç»œä¸­çš„æ¢¯åº¦æ¸…é›¶
            optimizer.zero_grad()

            # è¿›è¡Œä¸€æ¬¡æ¨ç†
            output = model(data)
            # output.shape = (b, 10)

            # è°ƒç”¨æŸå¤±å‡½æ•°ï¼Œè®¡ç®—æŸå¤±å€¼
            loss = loss_func(output, target)
            train_loss += loss.item()

            # åå‘ä¼ æ’­ï¼Œè®¡ç®—lossçš„æ¢¯åº¦
            loss.backward()

            # ä½¿ç”¨ç½‘ç»œä¸­çš„æ¢¯åº¦æ›´æ–°å‚æ•°
            optimizer.step()

            # æ¯100æ¬¡å¾ªç¯æ‰“å°ä¸€æ¬¡
            if batch_idx % 100 == 0:
                print(f"è®­ç»ƒè½®æ¬¡: {epoch + 1} [{batch_idx * len(data)}/{len(train_loader.dataset)}] æŸå¤±: {loss.item():.6f}")
        
        print(f"================ è®­ç»ƒè½®æ¬¡: {epoch + 1} å¹³å‡æŸå¤±: {train_loss / len(train_loader):.6f} ================\n")


if __name__ == '__main__':
    main()

 ```
 3.8
 [ä¸‹è½½å‹ç¼©åŒ…3.8.zip](/courses_resource/ai_mllm_aigc/resource/src/3.8.zip)
 3.10.py
```
from torch import nn


def vgg_block(num_convs, in_channels, out_channels):
    layers = []
    for _ in range(num_convs):
        layers.append(nn.Conv2d(in_channels, out_channels,
                                kernel_size=3, padding=1))
        layers.append(nn.ReLU())
        in_channels = out_channels
    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))
    return nn.Sequential(*layers)

def vgg(conv_arch):
    conv_blks = []
    in_channels = 3
    # å·ç§¯å±‚éƒ¨åˆ†
    for (num_convs, out_channels) in conv_arch:
        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))
        in_channels = out_channels

    return nn.Sequential(
        *conv_blks, nn.Flatten(),
        # å…¨è¿æ¥å±‚éƒ¨åˆ†
        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),
        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),
        # é¢„æµ‹ 1000 ä¸ªç±»åˆ«
        nn.Linear(4096, 1000)
    )

conv_arch = ((2, 64), (2, 128), (3, 256), (3, 512), (3, 512))
net = vgg(conv_arch)

```
3.11.py
```
from torch import nn

# åœ¨å…¨è¿æ¥å±‚ä½¿ç”¨ BN
layer = nn.Sequential(
    nn.Linear(128, 64),
    # 1D Batch normï¼Œè¾“å…¥ç»´åº¦ä¸º 256
    nn.BatchNorm1d(64),
    nn.ReLU()
)

# åœ¨å·ç§¯å±‚ä½¿ç”¨ BN
layer = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=3),
    # 2D Batch normï¼Œé€šé“æ•°ä¸º 6ï¼ˆå·ç§¯å±‚çš„è¾“å‡ºé€šé“æ•°ï¼‰
    nn.BatchNorm2d(6),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size = 2, stride = 2)
)

```
3.13.py
```
from torch import nn
from torch.nn import functional as F


class Residual(nn.Module):
    """å®ç°æ®‹å·®å—
    """
    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):
        super().__init__()
        self.seq = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels)
        )
        
        # æ˜¯å¦ä½¿ç”¨ 1x1 å·ç§¯å±‚æ¥é€‚é…å°ºå¯¸
        if use_1x1conv:
            self.res_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)
        else:
            self.res_conv = None

    def forward(self, X):
        Y = self.seq(X)
        
        if self.res_conv:
            X = self.res_conv(X)
        
        Y += X
        return F.relu(Y)

```

 [ä¸‹è½½æµ‹è¯•æ•°æ®å‹ç¼©åŒ…](/courses_resource/ai_mllm_aigc/resource/src/data.zip)

# è¯¾ç¨‹å­¦ä¹ PPTèµ„æ–™
 
[å¾ªç¯ç¥ç»ç½‘ç»œ-RNN.pdf](/courses_resource/ai_mllm_aigc/resource/å¾ªç¯ç¥ç»ç½‘ç»œ-RNN.pdf)
[å¾ªç¯ç¥ç»ç½‘ç»œ-LSTM.pdf](/courses_resource/ai_mllm_aigc/resource/å¾ªç¯ç¥ç»ç½‘ç»œ-LSTM.pdf)
[5.3å¾ªç¯ç¥ç»ç½‘ç»œ-GRU.pdf](/courses_resource/ai_mllm_aigc/resource/5.3å¾ªç¯ç¥ç»ç½‘ç»œ-GRU.pdf)
[6.1Transformeræ¶æ„è§£æ.pdf](/courses_resource/ai_mllm_aigc/resource/6.1Transformeræ¶æ„è§£æ.pdf)
[7.1 BERT.pdf](/courses_resource/ai_mllm_aigc/resource/7.1BERT.pdf)
[LLaMA3.pdf](/courses_resource/ai_mllm_aigc/resource/LLaMA3.pdf)
[GPT.pdf](/courses_resource/ai_mllm_aigc/resource/GPT.pdf)